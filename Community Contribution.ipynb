{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group 19: Long Lin & Shiang Xuanyuan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description\n",
    "There is so much information on the Internet that a human being can’t master it all in a lifetime. What we need to do is not access to this information but using an extensible way to collect, organize, and analyze it. Imagine you have to pull a large amount of data from websites and you want to do it as quickly as possible. How would you do it without manually going to each website and getting the data? Well, “Web Scraping” is the answer. Web Scraping just makes this job easier and faster. \n",
    "\n",
    "Although we could use web scraping in R, the most common and easiest way of retrieving data from internet is using Python. We would give a brief introduction on how to use Python to do the web scraping, storing data into a csv file, then we could use any another languages to do the data analysis, data visualization, etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why is Web Scraping Used?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Web scraping is used to collect large information from websites. But why does someone have to collect such large data from websites? To know about this, let’s look at the applications of web scraping:\n",
    "* **Price Comparison:** Services such as ParseHub use web scraping to collect data from online shopping websites and use it to compare the prices of products.\n",
    "* **Email address gathering:** Many companies that use email as a medium for marketing, use web scraping to collect email ID and then send bulk emails.\n",
    "* **Social Media Scraping:** Web scraping is used to collect data from Social Media websites such as Twitter to find out what’s trending.\n",
    "* **Research and Development:** Web scraping is used to collect a large set of data (Statistics, General Information, Temperature, etc.) from websites, which are analyzed and used to carry out Surveys or for R&D.\n",
    "* **Job listings:** Details regarding job openings, interviews are collected from different websites and then listed in one place so that it is easily accessible to the user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Web Scraping?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Web scraping is an automated method used to extract large amounts of data from websites. The data on the websites are unstructured. Web scraping helps collect these unstructured data and store it in a structured form. There are different ways to scrape websites such as online Services, APIs or writing your own code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Markdown Logo is here.](https://www.edureka.co/blog/wp-content/uploads/2018/11/Untitled-1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to Scrape Data From A Website?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you run the code for web scraping, a request is sent to the URL that you have mentioned. As a response to the request, the server sends the data and allows you to read the HTML or XML page. The code then, parses the HTML or XML page, finds the data and extracts it. \n",
    "\n",
    "To extract data using web scraping with python, you need to follow these basic steps:\n",
    "\n",
    "1. Find the URL that you want to scrape\n",
    "2. Inspecting the Page\n",
    "3. Find the data you want to extract\n",
    "4. Write the code\n",
    "5. Run the code and extract the data\n",
    "6. Store the data in the required format \n",
    "\n",
    "**Note:** Before we extract data from a website, it would be better to check whether a website allows web scraping or not, you can look at the website’s “robots.txt” file. You can find this file by appending “/robots.txt” to the URL that you want to scrape."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries used for Web Scraping \n",
    "As we know, Python is has various applications and there are different libraries for different purposes. In our further demonstration, we will be using the following libraries:\n",
    "\n",
    "* **Requests:** the tool of website downloading\n",
    "* **BeautifulSoup:** Beautiful Soup is a Python package for parsing HTML and XML documents. It creates parse trees that is helpful to extract the data easily.\n",
    "* **Pandas:** Pandas is a library used for data manipulation and analysis. It is used to extract the data and store it in the desired format. \n",
    "\n",
    "There are several libraries may also be used in web scraping:\n",
    "* **Re:** regular expression, used to data cleaning and organizing.\n",
    "* **Selenium:**  Selenium is a web testing library. It is used to automate browser activities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get started with scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HTML Format\n",
    "Before we move the coding part, it would be necessarily to understand the formats of HTML and some rules of scraping. For example, the following is a piece of a html document:\n",
    "\n",
    "```\n",
    "<html><head><title>The Dormouse's story</title></head>\n",
    "<body>\n",
    "<p class=\"title\"><b>The Dormouse's story</b></p>\n",
    "<p class=\"story\">Once upon a time there were three little sisters; and their names were\n",
    "<a href=\"http://www.jb51.net\" class=\"sister\" id=\"link1\">Elsie</a>,\n",
    "<a href=\"http://www.jb51.net\" class=\"sister\" id=\"link2\">Lacie</a> and\n",
    "<a href=\"http://www.jb51.net\" class=\"sister\" id=\"link3\">Tillie</a>;\n",
    "and they lived at the bottom of a well.</p>\n",
    "<p class=\"story\">...</p>\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that every <tag>  services a part of the website:\n",
    "    \n",
    "1. HTML documents are stored between `<html>` and `</html>`.\n",
    "2. The meta statements and script statements must be between `<head>` and `</head>`.\n",
    "3. The visible parts of HTML documents are between `<body>` and `</body>`.\n",
    "4. `<p>` is defined as the paragraph of article.\n",
    "5. `<a>` indicates the hyperlink.\n",
    "6. Attribute - `id`: is unique for every HTML tag, and the array in `id` must be unique in the entire HTML document.\n",
    "7. Attribute - `class`: is used to style labels with the same value.\n",
    "8. We would use `id` and `class` to locate the information we want.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web Scraping Examples:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Basics of Web Scraping with BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Loading Web Pages with 'request'\n",
    "The requests module allows you to send HTTP requests using Python.\n",
    "\n",
    "The HTTP request returns a Response Object with all the response data (content, encoding, status, and so on). One example of getting the HTML of a page <a href=\"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\" target=\"_blank\">this link</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'<!DOCTYPE html><html lang=\"en\" class=\"text-gray-500 antialiased bg-white js-focus-visible\"><head><style>@font-face {\\n    font-fa'\n",
      "b'mily: Inter var;\\n    font-weight: 100 900;\\n    font-display: swap;\\n    font-style: normal;\\n    font-named-instance:\"Regular\";\\n\\tf'\n",
      "b'ont-feature-settings: \"cv02\",\"cv03\",\"cv04\",\"cv11\";\\n    src: url(/assets/fonts/inter-var.woff2) format(\"woff2\")\\n}\\n\\n@font-face {'\n",
      "b'font-family: Inter var;\\n    font-weight: 100 900;\\n    font-display: swap;\\n    font-style: italic;\\n    font-named-instance:\"It'\n",
      "b'alic\";\\n\\tfont-feature-settings: \"cv02\",\"cv03\",\"cv04\",\"cv11\";\\n    src: url(/assets/fonts/inter-var2.woff2) format(\"woff2\")\\n}\\n</sty'\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Print all outcomes\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "\n",
    "# Make a request to URL\n",
    "# Store the result in 'res' variable\n",
    "res = requests.get('https://codedamn.com')\n",
    "\n",
    "# Store the text response in a variable called txt\n",
    "txt = res.text\n",
    "# Store the status code in a variable called status\n",
    "status = res.status_code\n",
    "\n",
    "# Print the first five line\n",
    "line_nu = 0\n",
    "for line in res:\n",
    "    if line_nu<5:\n",
    "        print(line.strip())\n",
    "        line_nu += 1\n",
    "    else:\n",
    "        break\n",
    "\n",
    "#print(res.text)\n",
    "#print(res.status_code)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Extracting title with BeautifulSoup\n",
    "Some features that make BeautifulSoup a powerful solution are:\n",
    "\n",
    "1. It provides a lot of simple methods and Pythonic idioms for navigating, searching, and modifying a DOM tree. It doesn't take much code to write an application\n",
    "2. Beautiful Soup sits on top of popular Python parsers like lxml and html5lib, allowing you to try out different parsing strategies or trade speed for flexibility.\n",
    "\n",
    "Basically, BeautifulSoup can parse anything on the web you give it.\n",
    "\n",
    "Here’s a simple example of BeautifulSoup (getting the HTML of a page <a href=\"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\" target=\"_blank\">this link</a>):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "codedamn Web Scraper demo\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Make a request to the goal URL\n",
    "page = requests.get(\n",
    "    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\")\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "# Use BeautifulSoup to store the title of this page into a variable called page_title\n",
    "# Extract title of page\n",
    "page_title = soup.title.text\n",
    "\n",
    "# print the result\n",
    "print(page_title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Soup-ed body and head\n",
    "Store body content (without calling .text) of URL in page_body\n",
    "\n",
    "Store head content (without calling .text) of URL in page_head\n",
    "\n",
    "When you try to print the page_body or page_head you'll see that those are printed as strings. But in reality, when you print(type page_body) you'll see it is not a string but it works fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<title>codedamn Web Scraper demo</title> <head>\n",
      "<!-- Anti-flicker snippet (recommended)  -->\n",
      "<style>\n",
      "\t\t\t.async-hide {\n",
      "\t\t\t\topacity: 0 !important;\n",
      "\t\t\t}\n",
      "\t\t</style>\n",
      "<title>codedamn Web Scraper demo</title>\n",
      "<meta charset=\"utf-8\"/>\n",
      "<meta content=\"IE=edge,chrome=1\" http-equiv=\"X-UA-Compatible\"/>\n",
      "<meta content=\"web scraping,Web Scraper,Chrome extension,Crawling,Cross platform scraper, \" name=\"keywords\"/>\n",
      "<meta content=\"The most popular web scraping website.\" name=\"description\"/>\n",
      "<link href=\"/webscraper-python-codedamn-classroom-website/favicon.png\" rel=\"icon\" sizes=\"128x128\"/>\n",
      "<meta content=\"width=device-width, initial-scale=1.0\" name=\"viewport\"/>\n",
      "<link href=\"/webscraper-python-codedamn-classroom-website/app.css\" rel=\"stylesheet\"/>\n",
      "<link href=\"/webscraper-python-codedamn-classroom-website/logo-icon.png\" rel=\"apple-touch-icon\"/>\n",
      "<script defer=\"\" src=\"/webscraper-python-codedamn-classroom-website/app.js\"></script>\n",
      "</head>\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Make a request\n",
    "page = requests.get(\n",
    "    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\")\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "# Extract title of page\n",
    "page_title = soup.title\n",
    "\n",
    "# Extract body of page\n",
    "page_body = soup.body\n",
    "\n",
    "# Extract head of page\n",
    "page_head = soup.head\n",
    "\n",
    "# print the result\n",
    "print(page_title, page_head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4: select with BeautifulSoup\n",
    "Now that we have explored some parts of BeautifulSoup, let's look how we can select DOM elements with BeautifulSoup methods.\n",
    "\n",
    "Once we have the `soup` variable (like previous labs), we can work with `.select` on it which is a CSS selector inside BeautifulSoup. That is, you can reach down the DOM tree just like how we will select elements with CSS. `.select` returns a Python list of all the elements. This is why we selected only the first element here with the `[0]` index. Let's look at an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Test Sites', 'E-commerce training site'] 7 reviews\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "# Make a request\n",
    "page = requests.get(\n",
    "    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\")\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "# Create all_h1_tags as empty list\n",
    "all_h1_tags = []\n",
    "\n",
    "# Set all_h1_tags to all h1 tags of the soup\n",
    "for element in soup.select('h1'):\n",
    "    all_h1_tags.append(element.text)\n",
    "\n",
    "# Create seventh_p_text and set it to 7th p element text of the page\n",
    "seventh_p_text = soup.select('p')[6].text\n",
    "\n",
    "print(all_h1_tags, seventh_p_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 5: Top items being scraped right now\n",
    "Let's go ahead and extract the top items scraped from the URL: https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\n",
    "\n",
    "If we open this page in a new tab, we’ll see some top items.Now we need to scrape out their names and store them in a list called `top_items`. We will also extract out the reviews for these items as well.\n",
    "\n",
    "In the following process:\n",
    "1. First of all we select all the `div.thumbnail` elements which gives us a list of individual products\n",
    "2. Then we iterate over them\n",
    "3. Because `select` allows us to chain over itself, we can use select again to get the title.\n",
    "4. Note that because we're running inside a loop for `div.thumbnail` already, the `h4 > a.title` selector would only give us one result, inside a list. We select that list's 0th element and extract out the text.\n",
    "5. Finally we strip any extra whitespace and append it to our list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'title': 'Asus AsusPro Adv...', 'review': '7 reviews'}, {'title': 'Asus ROG Strix G...', 'review': '4 reviews'}, {'title': 'Acer Aspire 3 A3...', 'review': '2 reviews'}]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "# Make a request\n",
    "page = requests.get(\n",
    "    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\")\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "# Create top_items as empty list\n",
    "top_items = []\n",
    "\n",
    "# Extract and store in top_items according to instructions on the left\n",
    "products = soup.select('div.thumbnail')\n",
    "for elem in products:\n",
    "    title = elem.select('h4 > a.title')[0].text\n",
    "    review_label = elem.select('div.ratings')[0].text\n",
    "    info = {\n",
    "        \"title\": title.strip(),\n",
    "        \"review\": review_label.strip()\n",
    "    }\n",
    "    top_items.append(info)\n",
    "\n",
    "print(top_items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 6: Extracting Links\n",
    "So far we have seen how we can extract the text, or rather innerText of elements. Let's now see how we can extract attributes by extracting links from the page.\n",
    "\n",
    "Here, we extract the `href` attribute just like you did in the image case. The only thing we're doing is also checking if it is None. We want to set it to empty string, otherwise we want to strip the whitespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'href': '', 'text': 'Toggle navigation'}, {'href': '/webscraper-python-codedamn-classroom-website/', 'text': ''}, {'href': '#page-top', 'text': ''}, {'href': '/webscraper-python-codedamn-classroom-website/', 'text': 'Web Scraper'}, {'href': '/webscraper-python-codedamn-classroom-website/cloud-scraper', 'text': 'Cloud Scraper'}, {'href': '/webscraper-python-codedamn-classroom-website/pricing', 'text': 'Pricing'}, {'href': '#section3', 'text': 'Learn'}, {'href': '/webscraper-python-codedamn-classroom-website/documentation', 'text': 'Documentation'}, {'href': '/webscraper-python-codedamn-classroom-website/tutorials', 'text': 'Video Tutorials'}, {'href': '/webscraper-python-codedamn-classroom-website/how-to-videos', 'text': 'How to'}, {'href': '/webscraper-python-codedamn-classroom-website/test-sites', 'text': 'Test Sites'}, {'href': 'https://forum.webscraper.io/', 'text': 'Forum'}, {'href': 'https://chrome.google.com/webstore/detail/web-scraper/jnhgnonknehpejjnehehllkliplmbmhn?hl=en', 'text': 'Install'}, {'href': 'https://cloud.webscraper.io/', 'text': 'Login'}, {'href': '/webscraper-python-codedamn-classroom-website/test-sites/e-commerce/allinone', 'text': 'Home'}, {'href': '/webscraper-python-codedamn-classroom-website/test-sites/e-commerce/allinone/computers', 'text': 'Computers'}, {'href': '/webscraper-python-codedamn-classroom-website/test-sites/e-commerce/allinone/phones', 'text': 'Phones'}, {'href': '/webscraper-python-codedamn-classroom-website/test-sites/e-commerce/allinone/product/593', 'text': 'Asus AsusPro Adv...'}, {'href': '/webscraper-python-codedamn-classroom-website/test-sites/e-commerce/allinone/product/583', 'text': 'Asus ROG Strix G...'}, {'href': '/webscraper-python-codedamn-classroom-website/test-sites/e-commerce/allinone/product/576', 'text': 'Acer Aspire 3 A3...'}, {'href': '/webscraper-python-codedamn-classroom-website/', 'text': 'Web Scraper browser extension'}, {'href': '/webscraper-python-codedamn-classroom-website/pricing', 'text': 'Web Scraper Cloud'}, {'href': '/webscraper-python-codedamn-classroom-website/contact', 'text': 'Contact'}, {'href': '/webscraper-python-codedamn-classroom-website/privacy-policy', 'text': 'Website Privacy Policy'}, {'href': '/webscraper-python-codedamn-classroom-website/extension-privacy-policy', 'text': 'Browser Extension Privacy Policy'}, {'href': 'http://webscraperio.us-east-1.elasticbeanstalk.com/downloads/Web_Scraper_Media_Kit.zip', 'text': 'Media kit'}, {'href': '/webscraper-python-codedamn-classroom-website/jobs', 'text': 'Jobs'}, {'href': '/webscraper-python-codedamn-classroom-website/blog', 'text': 'Blog'}, {'href': '/webscraper-python-codedamn-classroom-website/documentation', 'text': 'Documentation'}, {'href': '/webscraper-python-codedamn-classroom-website/tutorials', 'text': 'Video Tutorials'}, {'href': '/webscraper-python-codedamn-classroom-website/screenshots', 'text': 'Screenshots'}, {'href': '/webscraper-python-codedamn-classroom-website/test-sites', 'text': 'Test Sites'}, {'href': 'https://forum.webscraper.io/', 'text': 'Forum'}, {'href': 'mailto:info@webscraper.io', 'text': 'info@webscraper.io'}, {'href': 'https://www.facebook.com/webscraperio/', 'text': ''}, {'href': 'https://twitter.com/webscraperio', 'text': ''}, {'href': '#', 'text': 'Web Scraper'}]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "# Make a request\n",
    "page = requests.get(\n",
    "    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\")\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "# Create top_items as empty list\n",
    "all_links = []\n",
    "\n",
    "# Extract and store in top_items according to instructions on the left\n",
    "links = soup.select('a')\n",
    "for ahref in links:\n",
    "    text = ahref.text\n",
    "    text = text.strip() if text is not None else ''\n",
    "\n",
    "    href = ahref.get('href')\n",
    "    href = href.strip() if href is not None else ''\n",
    "    all_links.append({\"href\": href, \"text\": text})\n",
    "\n",
    "print(all_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 7: Generating CSV from data\n",
    "Finally, let's understand how we can generate CSV from a set of data. We will create a CSV with the following headings:\n",
    "1. Product Name\n",
    "2. Price\n",
    "3. Description\n",
    "4. Reviews\n",
    "5. Product Image\n",
    "These products are located in the `div.thumbnail`.\n",
    "* Product Name is the whitespace trimmed version of the name of the item (example - Asus AsusPro Adv..)\n",
    "* Price is the whitespace trimmed but full price label of the product (example - 1101.83)\n",
    "* The description is the whitespace trimmed version of the product description (example - Asus AsusPro Advanced BU401LA-FA271G Dark Grey, 14\", Core i5-4210U, 4GB, 128GB SSD, Win7 Pro)\n",
    "* Reviews are the whitespace trimmed version of the product (example - 7 reviews)\n",
    "* Product image is the URL (src attribute) of the image for a product (example - /webscraper-python-codedamn-classroom-website/cart2.png)\n",
    "* The name of the CSV file should be products.csv and should be stored in the same directory as our script.py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "# Make a request\n",
    "page = requests.get(\n",
    "    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\")\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "# Create top_items as empty list\n",
    "all_products = []\n",
    "\n",
    "# Extract and store in top_items according to instructions on the left\n",
    "products = soup.select('div.thumbnail')\n",
    "for product in products:\n",
    "    name = product.select('h4 > a')[0].text.strip()\n",
    "    description = product.select('p.description')[0].text.strip()\n",
    "    price = product.select('h4.price')[0].text.strip()\n",
    "    reviews = product.select('div.ratings')[0].text.strip()\n",
    "    image = product.select('img')[0].get('src')\n",
    "\n",
    "    all_products.append({\n",
    "        \"name\": name,\n",
    "        \"description\": description,\n",
    "        \"price\": price,\n",
    "        \"reviews\": reviews,\n",
    "        \"image\": image\n",
    "    })\n",
    "\n",
    "\n",
    "keys = all_products[0].keys()\n",
    "\n",
    "with open('products.csv', 'w', newline='') as output_file:\n",
    "    dict_writer = csv.DictWriter(output_file, keys)\n",
    "    dict_writer.writeheader()\n",
    "    dict_writer.writerows(all_products)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `for` block is the most interesting here. We extract all the elements and attributes from what we've learned so far in all the labs.\n",
    "\n",
    "When we run this code, we end up with a nice CSV file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 8: Present the result of CSV from Part7:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "      <th>price</th>\n",
       "      <th>reviews</th>\n",
       "      <th>image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Asus AsusPro Adv...</td>\n",
       "      <td>Asus AsusPro Advanced BU401LA-FA271G Dark Grey...</td>\n",
       "      <td>$1139.54</td>\n",
       "      <td>7 reviews</td>\n",
       "      <td>/webscraper-python-codedamn-classroom-website/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Asus ROG Strix G...</td>\n",
       "      <td>Apple MacBook Air 13.3\", Core i5 1.8GHz, 8GB, ...</td>\n",
       "      <td>$1101.83</td>\n",
       "      <td>4 reviews</td>\n",
       "      <td>/webscraper-python-codedamn-classroom-website/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Acer Aspire 3 A3...</td>\n",
       "      <td>Acer Aspire 3 A315-51 Black, 15.6\" FHD, Core\\n...</td>\n",
       "      <td>$494.71</td>\n",
       "      <td>2 reviews</td>\n",
       "      <td>/webscraper-python-codedamn-classroom-website/...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  name                                        description  \\\n",
       "0  Asus AsusPro Adv...  Asus AsusPro Advanced BU401LA-FA271G Dark Grey...   \n",
       "1  Asus ROG Strix G...  Apple MacBook Air 13.3\", Core i5 1.8GHz, 8GB, ...   \n",
       "2  Acer Aspire 3 A3...  Acer Aspire 3 A315-51 Black, 15.6\" FHD, Core\\n...   \n",
       "\n",
       "      price    reviews                                              image  \n",
       "0  $1139.54  7 reviews  /webscraper-python-codedamn-classroom-website/...  \n",
       "1  $1101.83  4 reviews  /webscraper-python-codedamn-classroom-website/...  \n",
       "2   $494.71  2 reviews  /webscraper-python-codedamn-classroom-website/...  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"products.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why is Python Good for Web Scraping?\n",
    "Here is the list of features of Python which makes it more suitable for web scraping.\n",
    "\n",
    "* **Ease of Use:** Python is simple to code. You do not have to add semi-colons “;” or curly-braces “{}” anywhere. This makes it less messy and easy to use.\n",
    "* **Large Collection of Libraries:** Python has a huge collection of libraries such as Numpy, Matlplotlib, Pandas etc., which provides methods and services for various purposes. Hence, it is suitable for web scraping and for further manipulation of extracted data.\n",
    "* **Dynamically typed:** In Python, you don’t have to define datatypes for variables, you can directly use the variables wherever required. This saves time and makes your job faster.\n",
    "* **Easily Understandable Syntax:** Python syntax is easily understandable mainly because reading a Python code is very similar to reading a statement in English. It is expressive and easily readable, and the indentation used in Python also helps the user to differentiate between different scope/blocks in the code. \n",
    "* **Small code, large task:** Web scraping is used to save time. But what’s the use if you spend more time writing the code? Well, you don’t have to. In Python, you can write small codes to do large tasks. Hence, you save time even while writing the code.\n",
    "* **Community:** What if you get stuck while writing the code? You don’t have to worry. Python community has one of the biggest and most active communities, where you can seek help from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Techniques for Web Scraping\n",
    "BeautifulSoup is beginner-friendly and well-documented, the size of web scraping project supported can be large or small. However, there still have several options for web scraping:\n",
    "1. **Scrapy:** Scrapy is a fast high-level web crawling and web scraping framework, used to crawl websites and extract structured data from their pages.\n",
    "2. **Selenium:** simulate a request in the browser and wait for the dynamic content to be displayed, by using selenium package in Python, the path of ChromeDriver is required.\n",
    "3. Try to integrate some public APIs into code, Data retrieval is much more efficient than page crawling.\n",
    "4. When the size of dataset is too large, consider using **MySQL** or **MongoDB** to store the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referrences:\n",
    "1. https://www.edureka.co/blog/web-scraping-with-python/\n",
    "2. https://www.freecodecamp.org/news/how-to-scrape-websites-with-python-2/\n",
    "3. https://www.freecodecamp.org/news/web-scraping-python-tutorial-how-to-scrape-data-from-a-website/\n",
    "4. https://oxylabs.io/blog/python-web-scraping\n",
    "5. https://gist.github.com/gjreda/f3e6875f869779ec03db\n",
    "6. https://www.edureka.co/blog/wp-content/uploads/2018/11/Untitled-1.jpg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
